---
layout: cnblog_post
title:  "celery"
permalink: '/misc/celery'
date:   2018-12-09 07:34:39
categories: misc
---

<img src="https://raw.githubusercontent.com/iampythoner/iampythoner.github.io/master/static/img/AMQP%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E5%9B%BE.png" width="500" alt="AMQP"/>

<img src="https://raw.githubusercontent.com/iampythoner/iampythoner.github.io/master/static/img/Celery%E6%9E%B6%E6%9E%84%E5%9B%BE.png" width="500" alt="Celery架构图"/>

#### 基本使用

```
In : from proj.tasks import add
In : r = add.delay(1, 3)
In : r
Out: <AsyncResult: xxxxx>

In : r.result
Out: 4

In : r.status
Out: u'SUCCESS'

In : r.successful()
Out: True

In : r.backend
Out: <celery.backends.redis.RedisBackedn at 0x78943> # 保存在redis中


In : task_id = 'xxxxxx'
In : add.AsyncResult(task_id).get() # 获取指定task_id的结果
Out: 4

# 或者
In : from celery.result import AsyncResult
In : AsyncResult(task_id).get()
Out: 4
```

#### 使用任务调度 beat

```
CELERYBEAT_SCHEDUEL = { # 4.0之后使用名称： beat_schedule
    "task": 'projb.tasks.add',
    'schedule': timedelta(seconds=10),
    'args': (16, 16)
}
```

指定了任务每10秒跑一次，同时指定了参数16 和 16，

启动beat:

```
celery beat -A projb
```
启动前，先启动worker。

Beat 和 worker 可以一起启动(但是celery不建议这样使用[http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html#starting-the-scheduler](http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html#starting-the-scheduler))：

```
celery -B -A projb worker -l info
```

在使用beat定时任务时，可以在beat任务执行时调用其他task，这种模式非常有用(比如定时在某个时间，下载一些文件,可以将下载任务做成task),


另外, `schedule`参数的类型选择是多样的，celery目前支持 `integer`(直接设置每隔多少秒执行)、`timedelta`、`celery.schedules.crontab`[http://docs.celeryproject.org/en/latest/reference/celery.schedules.html#celery.schedules.crontab](http://docs.celeryproject.org/en/latest/reference/celery.schedules.html#celery.schedules.crontab) 、 `celery.schedules.solar`[http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html#solar-schedules](http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html#solar-schedules)

`relative`参数也非常关键:

```
# relative 参数
default: false 依据celery  beat 启动的时间
true 依据时钟，会在整点时刻执行

另外要注意: 在relative值为默认的false的情况下
分钟级间隔的任务：启动后直接执行一次
小时级间隔的任务：启动之后间隔时间到了才执行

另外: supervisor管理的celery发现reload配置文件之后，不算重启celery，这个有待进一步研究。
还有一个发现：设置了一个1小时执行一次的任务A(relative=True),设置了一个每隔5分钟执行的任务B(relative=True)
启动beat之后B任务都是在5的倍数分钟时刻执行，这都正常。之后经过一个整点，在13:00需要执行一次任务A，此时任务B并没有执行，
而是在13:01执行，之后也是每隔5分钟执行(13:06, 13:11), 目前猜测这是celery的一个优化，但是这个等待时间1分钟如何计算，
是不是根据任务A的执行时间决定的，有待进一步研究。
```


#### 任务绑定、记录日志和重试

任务绑定、记录日志和重试是Celery常用的3个高级属性。现在修改proj/tasks.py文件，添加div函数用于演示:

```
from celery.utils.log import get_task_logger

logger = get_task_logger(_ name_ _)

@app.task(bind=True)
def div(self, x, y):
    logger.info(('Executing task id {0.id}, args: {0.args!r}'
    'kwargs: {0.kwargs!r}'). format(self.request))

    try:
        result = x / y
    except ZeroDivisionError as e:
        raise self.retry(exc=e, countdown=5, max_retries=3)
    return result
```

当使用bind = True后，函数的参数发生变化，多出了参数self(第一个参数)， 相当于把变成了一个已绑定的方法，通过self可以获得任务的上下文。<br>
在IPython中调用div:

```
In : from proj.tasks import div
In : r = div.delay(2, 1)
```

可以看到如下执行信息:

```
[2016-06-03 15:50:32,853: INFO/Worker-1] proj.tasks.div[71606-835-1311282-45-856B8-2]: Executing task id 71606-835-1311282-45-856B8-2, args: [2, 1] kwargs {}
```

换成能造成异常的参数:

```
In: r = div.delay(2, 0)
```

可以发现每5秒就会重试一次， -共重试3次(默认重复3次),然后抛出异常。


#### 任务调用

之前我们调用任务都使用`delay`方法，格式如下:

```
task.delay(arg1, arg2, kwarg1='x', kwarg2='y')
```

`delay`其实是`apply_async`的别名，还可以使用如下方法调用

```
task.apply_async(args=[arg1, arg2], kwargs={'kwarg1':'x', 'kwarg2'='y'}) 
```

二者的区别在于参数的表达方式，而且apply_async支持更多的参数:

```
1. countdown: 等待-段时间再执行。
add.apply_async((2, 2)， countdown=5) # 5秒后再执行

2. eta: ETA是estimated time of arrival 的缩写，也就是定义了任务的开始时间。countdown相当于特殊的单位为秒的eta。
add.apply_ async((2, 2)， eta=now + timedelta(seconds=10)) # 时间到了now加了10秒，任务才开始

# 定点发送任务时一定要注意时区，celery worker真正执行的时刻会根据datetime类型的时间计算来， （eta设置的任务会提前发送给worker让worker到时间执行）
task_execute_time = datetime.now() + timedelta(1) # 任务执行的时间
celery_time = datetime.fromtimestamp(time.mktime(task_execute_time.timetuple()), tz=self.app.timezone)
xxxx_task.apply_async(eta=celery_time)

eta的坑，要设置celery配置 broker_transport_options = {'visibility_timeout': timeout_value}
timeout_value以秒为单位，当前发送任务的时间和任务真正执行的时间间隔要小于timeout_value，参考：
https://www.jianshu.com/p/9e422d9f1ce2
https://blog.csdn.net/woshiaotian/article/details/36422781
https://github.com/celery/django-celery/issues/215#issuecomment-120763548
https://github.com/celery/celery/issues/2481

3. expires: 设置超时时间。
add.apply_async((2, 2), expires=60) # 60秒之后过期

4. retry: 定义如果任务失败是否重试。
add.apply_async((2, 2), retry=Fase) # 失败了不会重试

5. retry_policy:重试策略。 可以设置如下几个键。
```

| 键 | 含义 |
| ----- | ----- |
| max_retries | 最大重试次数，默认为3次 |
| interval_start | 重试等待的间隔秒数，默认为0.表示直接重试不等待 |
| interval_step | 每次重试让重试间隔增加的秒数，比如第1次重试间隔0秒，第二次重试间隔就是0.2秒，... 可以是数数字或者浮点数，默认值是0.2 |
| interval_max | 重试间隔最大的秒数。也就是通过interal_step增大到多少秒之后就不再增加了。可以是数字或者浮点数，默认值是0.2 |


使用apply_ async 也可以自定义发布者、交换机、路由键、队列、优先级(目前只支持Redis和Beanstalk)、序列方案和压缩方法:

```
add.apply_async((2, 2), compression= 'zlib',
    serializer= 'json', queue='priority.high',
    routing_key='web.add', priority=0, exchange='web_exchange')
```

#### 信号系统

参见 [http://docs.celeryproject.org/en/latest/userguide/signals.html#task-signals](http://docs.celeryproject.org/en/latest/userguide/signals.html#task-signals)


##### 任务信号

| 信号 | 含义 |
| ----- | ----- |
| before_task_publish | 任务发布前|
| after_task_publish | 任务发布后|
| task_prerun | 任务执行前 |
| task_postrun | 任务执行后 |
| task_retry | 任务重试时 |
| task_success | 任务完成时 | 
| task_failure | 任务失败时 |
| task_revoked | 任务撤销或终止 |
| task_unknown | worker接收到没有注册的消息 |
| task_rejected | 当worker接收到位置类型的消息时 |


```
from celery.signal import after_task_publish

@after_task_publish.connect
def task_sent_handler(sender=None, body=None, **kwargs):
    print('after_task_publish： task_id: {body[id]}; sender: {sender}').format(
        body=body, sender=sender
    )


# 结果
after_task_publish： task_id: xxxx; sender: proj.tasks.add
```
信号可以帮助我们了解任务执行情况，分析任务运行的瓶预。

##### 应用信号

##### worker信号

##### Beat信号

##### Eventlet信号

##### 日志信号

##### 命令信号

#### worker 管理

通常任务启动的方式：

```
/home/mike/.virtualenvs/xx_pro/bin/celery -A push.celeryapp worker -l info --concurrency=6 -Q group_send_queue,single_send_queue
```

--concurrency参数会忽略在初始化app时配置指定的`worker_concurrency`参数 <br>
-Q 指定的两个队列，前者为高优先级 （除了指定的队列的任务，其他任务这个worker不处理）<br>

>1.如果为在`app.conf.task_routes` 为任务指定了queue，那么这个任务只能在这个queue对应的worker中执行，因此启动worker时如果不指定对应的queue，那么这个worker不会处理这种任务<br>
>2.如果一个worker启动时指定了queue,那么它不会处理除了设置在这个queue以外的其他任务

想要以Daemon方式启动worker进程:

```
celery multi start label_name -A celeryapp -l info --pid=./celery_%n.pid --logfile=./celery_%n.log
```

其他格式符号:

| 参数 | 含义 |
| ----- | ----- |
| %n | 只包含主机名 |
| %h | 包含域名的主机名 |
| %d | 只包含域名 |
| %i | Prefork类型的进程索引，如果是主进程，则为0 |
| %I | 带分隔符的Prefork类型的进程索引。假设主进程是worker1，那么进程池的第一个进程为worker1-1 |

```
celery multi show label_name # 查看label_name 启动时的命令                          # 输出                                           
/path/to/bin/python -m celery worker --detach -n push@MikedeMBP.lan --pidfile=label_name.pid --logfile=label_name%I.log --executable=/path/to/bin/python 

celery multi names label_name # 获取label_name的节点名字
# 输出
label_name@MikedeMBP.lan

celery multi stop label_name # 停止label_name进程
# 输出
> Stopping nodes...
        > label_name@MikedeMBP.lan: TERM -> 17574
Could not signal label_name@MikedeMBP.lan (17574): No such process
> label_name@MikedeMBP.lan: DOWN

celery multi restart label_name  # 重新启动label_name 
# 输出
celery multi v4.2.1 (windowlicker)
> Stopping nodes...
        > label_name@MikedeMBP.lan: TERM -> 17574
Could not signal label_name@MikedeMBP.lan (17574): No such process
> Restarting node label_name@MikedeMBP.lan: OK


celery multi kill label_name   # 杀掉label_name 进程 （杀掉的是master进程，不要使用这个命令，因为使用之后master不在，无法管理其他worker进程，因此用stop命令也无法关闭所有的进程）    
celery multi v4.2.1 (windowlicker)
Sending KILL to node label_name@MikedeMBP.lan (20803)

```


#### 监控和管理celery

Celery提供了一此方便监控和管理的命令， 常用的命令有如下4个。

##### shell

```
1. shell: celery也提供了一个Python交互环境，内置了Celery 应用实例和全部已注册
的任务，支持默认的Python解释器、IPython 和BPython。

> celery shell -A proj
In [1]: celery
Out[1]: <Celery proj:8x7fad4aef8298>
In [2]: add.delay(1, 2) # 可以直接使用而不需要import

2. result: 通过task_id在命令行获得执行结果。
> celery -A proj result eba7fc8b-3c16-4337-998c-d53a4c176a493

3. inspect active:列出当前正在执行的任务。
> celery -A proj inspect active

4. inspect stats:列出Worker的统计数据，常用来查看配置是否正确以及系统的使用情况。
> celery -A proj inspect stats
```

Celery还可以撤销任务:

```
In : rs = add.delay(1, 2)
In : rs.revoke() # 只是撤销，如果任务已经在执行则撒销无效
In : rs.task_id
Out: d24a83e8-e985-48ef-adab-779559

In : app.control.revoke('d24a83e8-e985-48ef-adab-779559') # 通过task_id撤销
In : app.control.revoke('d24a83e8-e985-48ef-adab-779559', terminate=True) # 撤销正在执行的任务，默认使用TERM信号
In : app.control.revoke('d24a83e8-e985-48ef-adab-779559', terminate=True, signale='SIGKILL') # 撤销正在执行的任务，使用KILL信号
In : app.control.revoke([
...:  'd9878da5-9915-40a0-bfa1-392c7bde42ed',
...:  'd24a83e8-e985-48ef-adab 77944e5557',
...:  '444-d14-4e3d 9d2C8B6d'])  # 可以同时撒销多个任务
```

Worker进程在内存中保存了撒销的任务，一重启就会丢失，如果需要这个历史记录持久化，可以添加--statedb参数启动Worker: 

```
celery -A proj worker -l info --statedb=/tmp/worker.state
```

Celery官方推荐实时的Web监控工具Flower, 它实现下特性:

>●可以看到任务历史、任务具体的参数、开始时间等。<br>
>●提供图表和统计数据。<br>
>●实现全面的远程控制功能，包括但不限于撤销/终止任务、关闭和重启Worker进程、查看正在运行的任务等。<br>
>●提供一个HTTP API,方便集成到运维系统中。

同时，Celery 也自带了一个事件监控工具显示任务历史等信息，可以用来检查任务和跟踪错误:

```
celery -A projb events
```
需要注意是，要把设置`CELERY_SEND_TASK_SENT_EVENT`为True才可以获取事件. <br>
事件监控工具默认每秒都会刷新终端，从消息代理以celeryev开头的队列中找到执行任务的历史记录，可以使用键盘按键查看任务信息、错误堆栈、执行结果和撤销任务等。

#### 子任务

我们可以把任务通过签名的方法传给其他任务，成为一个子任务：

```
In : from celery import signature
In : task = signature('tasks.add', args=(2, 2), countdown=10)
Out: tasks.add(2, 2) # 通过签名生成了任务
In : task.apply_async()
Out: <AsnycResult: xxxx>
```
还可以通过如下方式创建子任务:

```
In : from proj.tasks import add
In : task = add.subtask((2, 2), countdown=10) #可以使用
In : task.apply_async()
0ut: <AsyncResult: a402fe1d-ed11-48f8-b163-8d7f65abBe3>
```

`add.subtask` 同样可以使用快捷方式`add.s(2, 2, countdown=10)`。<br>
子任务实现偏函数(partial)的方式非常有用，这种方式可以让任务在传递过程中才传入参数。

```
In : partial = add.s(2)
In : partialapply asyc((4,)) 
Out: <AsyncResult: b3aaa2ce-eddb-44eb-80fb-48ce5e36da00>
```
子任务支持如下5种原语来实现工作流。原语表示由若干条指令组成的，用于完成一定功能的过程。

##### 1. chain: 调用链。

前面的执行结果作为参数传给后面的任务，直到全部完成。

```
In : from celery import chain
In : res = chain(add.s(2, 2)，add.s(4), add.s(8))()
In : res.get()
Out: 16
```

chain也可以使用管道(\|):

```
In: (add.s(2, 2) | add.s(4) | ads(8))().get()
Out: 16
```

##### 2. group: 一次创建多个(一组)任务。

```
In : from celery import group
In : res = group(add.s(i, i) for i in range(10))()
In : res.get()
Out: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
```

##### 3. chord:等待任务全部完成时添加个回调任务。

```
In : res = chord((add.s(i, i) for i in range(1)), add.s(['a']))()
In : rs.get() # 执行完前面的循环，把结果拼成一个列表之后，再对这个列表添加'a'
0ut: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, u'a']
```

##### 4. map/starmap: 每个叁数都作为任务的参数执行一遍，map的参数只有一个，starmap支持的参数有多个

```
In: ~add.starmap(zip(range(10), range(10)))
0ut:[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
```
它相当于:

```
@app.task
def temp():
    return [add(i, i) for i in range(10)]
```
    
##### 5. chunks: 将任务分块


```
In: res = add.chunks(zip(range(50), range(50)), 10)()
In : res.get()
Out:
[[0, 2, 4, 6, 8, 10, 12, 14, 16, 18],
 [20, 22, 24, 26, 28, 30, 32, 34, 36, 38],
 [40, 42, 44, 46, 48, 50, 52, 54, 56, 58],
 [60, 62, 64, 66, 68, 70, 72, 74, 76, 78],
 [80, 82, 84, 86, 88, 90, 92, 94, 96, 98]]
```

#### 进阶篇: Celery 最佳实践

##### 使用自动扩展
多进程和Gevent模式的Worker支持自动扩展，通过--autoscale参数就可以实现:

```
celery -A proj worker -l info --autoscale=6,3
```

--autoscale参数接受2个数字。在上面的句子中，`autoscale=6,3`表示进程池平时保持3个进程，最大并发进程数可以达到6个。这里的取值和你的服务器CPU个数、任务闲忙程度、可用内存等指标有关。笔者个人的习惯是将最大并发数设置为CPU个数的2倍。理论上Worker进程数与CPU个数接近即可，但是也不尽然，它还与你的任务类型有关。比如笔者曾经遇到过的一个产品线的Worker数就比CPU数高很多，原因是每天上千万的任务中有大量的小任务，也有耗时比较久的任务，而且添加的任务会有瞬时高峰，比如每小时的整点、偶数小时的整点等。这很容易造成多个消费者在处理耗时的任务，让整体的及时性受到影响，而使用自动扩展可以比较好地解决这个问题。

##### 善用远程Debug
Celery 支持远程使用pdb调试任务，非常方便。

先添加一个任务:

```
from clery.contrib import rdb

@app.task
def sub(x, y):
    result = x - y
    rdb.set_trace() # 设置断点
    return result
```
重启Worker进程之后，使用Ipython发布任务:

```
In : from proj.tasks import sub
In : sub(2, 1)
Remote Debugger:6899, Please telnet into 127.0.0.1 6899.

Type 'exit' in ssion to continue.

Remote Debugger:6899 Waiting for client...
```

这个时候我们开启一个新的终端，使用tenet连接到6899端口

```
> telnet localhost 6899
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'
> /home/ubuntu/web develop//tepertstiono/proj/tasks.py(27)sub()
-> return result
(Pdb) result
1
(Pdb) x
2
(Pdb) continue
Connection closed by foreign host.
```

通过这样的方式，就不用在本地搭建Celery环境模拟任务失败的环境了。

##### 合理安排任务周期
如果项目中有很多调度模式的任务，就要合理安排这些定时任务的执行时间。很多人会懒，随便选择一些整点和凌晨期间作为任务执行时间。但是需要注意如下问题:<br>
1.需要和系统管理员和数据库管理员确认，确保你选择的时间段里面没有一些不合时宜的时间点。因为通常在凌晨，尤其是过了12点之后也是大量定时任务运行的时间，比如数据备份、生成前一天的数据报表这样的任务，如果选在凌晨执行你的任务，既可能让你的任务失败，也可能由于对数据库的访问压力让你和别人的任务花费更多的时间。<br>
2.选择非整点的任务执行周期。根据任务特性，可以把任务执行时间打散，尽量减少每小时里前45分钟Celery集群非常忙碌，后15分钟集群大部分空闲的情况。<br>
3.合理安排数据库、文件系统写任务。笔者的方式是把占用时间的数据库和文件系统写操作等任务安排在访问低峰期，并把写任务拆分后在分散的时间执行。

##### 合理使用队列和优先级
不要把任务都放在默认队列。合理安排任务的优先级和队列，让应该及时完成的任务不会因为某些原因被阻寒而影响用户体验。其次是合理使用apply_async 方法临时性地切换队列和优先级，提高整体的响应。
如果能明确知晓某些任务耗时很长，某些任务托时很短，至少可以按照任务花费的时间来把任务安排到不同的队列。

##### 保证业务逻辑的事务性
Celery 虽然提供错误重试机制，但是并没有提供任务的事务性。如果任务一部分执行成功之后失败，执行成功的那部分是没有回滚方法的，所以一开始就要在实现逻辑的时候对重试机制有明确的理解。

##### 关闭你不想要的功能
如果你对任务的执行结果没有兴趣，可以关闭它:

```
@app.task(ignore_result=True)
def mytask(...):
    doit()
```

如果项目不需要限速，可以设置`CELERY_DISABLE_RATE_LIMITS=True`直接全局地把这个特性关闭。

##### 使用阅后即焚模式

在使用队列的时候，默认使用持久化方式以确保任务被执行，如果你的任务不需要持久化，可以使用阅后即焚( transient)模式:

```
from kombu import Queue
Queue('transient', routing_key='transient', delivery_mode=1)
```

##### 善用Prefetch模式
Worker进程默认每次从消息代理获取`CELERYD_PREFETCH_MULTIPLIER`设置的任务数，假如你的任务都很细小，可以修改`CELERYD_PREFETCH_MULTIPLIER`的值，每次获取更多的任务数。这个值也是根据任务的执行情况决定的。

##### 善用工作流
如果一个任务有调用链，下一步任务需要等待上一步的结果，就不应该使用同步子任务。举一个抓取某电商网站的爬虫例子，它般分为以下几步:

```
1.获取要抓取的页面。
2.抓取对应页面。
3.解析页面数据。
4.把需要的数据存储到数据库和缓存中。
```

如果设计成下面的方式就是错误的:

```
@app.task
def page_crawler();
    url = get_url.delay().get()
    page = fetch_page.delay(url).get()
    info = parse_page.delay(page).get()
    store_page_info.delay(info)

@app.task
def get_url():
return PageInfo.objects.filter_by(need_upDate=True).first()

@app.task
def fetch_page(url):
    return requests.get(url)

@app.task
def parse_page(page):
    return myparser.parse_document(page)

@app.task
def store_page_info(info):
    return PageInfo.objects.create(info)
```
    
应该使用如下方式:

```
def page_crawler():
    # get_url -> fetch_page -> parse. page -> store page
    chain = get_url.s() I fetch_page.s() | parse_page.s() | store_page_info.s()chain()
```

控制任务的粒度不要太细。举个例子，假设现在每小时都要通过某网站的开放平台获取应的百万级条目的数据。如果为每个条目都创建一个任务显然是不合理的，应该更多地利用开放平台接口的限制。比如接口可以批量操作，一次最多取30个条目的数据，那么可考虑把单个任务定为处理30个条目的数据。

在生成任务的时候应该充分利用group/chain/chunks这些原语。



#######

```
[2018-12-19 16:46:23,142: CRITICAL/MainProcess] Unrecoverable error: AttributeError("'float' object has no attribute 'items'",)
Traceback (most recent call last):
  File "/home/mike/.pyenv/versions/3.6.6/envs/official_accounts/lib/python3.6/site-packages/celery/worker/worker.py", line 205, in start
    self.blueprint.start(self)
  File "/home/mike/.pyenv/versions/3.6.6/envs/official_accounts/lib/python3.6/site-packages/celery/bootsteps.py", line 119, in start
    step.start(parent)
  File "/home/mike/.pyenv/versions/3.6.6/envs/official_accounts/lib/python3.6/site-packages/celery/bootsteps.py", line 369, in start
    return self.obj.start()
  File "/home/mike/.pyenv/versions/3.6.6/envs/official_accounts/lib/python3.6/site-packages/celery/worker/consumer/consumer.py", line 317, in start
    blueprint.start(self)
  File "/home/mike/.pyenv/versions/3.6.6/envs/official_accounts/lib/python3.6/site-packages/celery/bootsteps.py", line 119, in start
    step.start(parent)
  File "/home/mike/.pyenv/versions/3.6.6/envs/official_accounts/lib/python3.6/site-packages/celery/worker/consumer/consumer.py", line 593, in start
    c.loop(*c.loop_args())
  File "/home/mike/.pyenv/versions/3.6.6/envs/official_accounts/lib/python3.6/site-packages/celery/worker/loops.py", line 91, in asynloop
    next(loop)
  File "/home/mike/.pyenv/versions/3.6.6/envs/official_accounts/lib/python3.6/site-packages/kombu/asynchronous/hub.py", line 354, in create_loop
    cb(*cbargs)
  File "/home/mike/.pyenv/versions/3.6.6/envs/official_accounts/lib/python3.6/site-packages/kombu/transport/redis.py", line 1040, in on_readable
    self.cycle.on_readable(fileno)
  File "/home/mike/.pyenv/versions/3.6.6/envs/official_accounts/lib/python3.6/site-packages/kombu/transport/redis.py", line 337, in on_readable
    chan.handlers[type]()
  File "/home/mike/.pyenv/versions/3.6.6/envs/official_accounts/lib/python3.6/site-packages/kombu/transport/redis.py", line 724, in _brpop_read
    self.connection._deliver(loads(bytes_to_str(item)), dest)
  File "/home/mike/.pyenv/versions/3.6.6/envs/official_accounts/lib/python3.6/site-packages/kombu/transport/virtual/base.py", line 983, in _deliver
    callback(message)
  File "/home/mike/.pyenv/versions/3.6.6/envs/official_accounts/lib/python3.6/site-packages/kombu/transport/virtual/base.py", line 632, in _callback
    self.qos.append(message, message.delivery_tag)
  File "/home/mike/.pyenv/versions/3.6.6/envs/official_accounts/lib/python3.6/site-packages/kombu/transport/redis.py", line 149, in append
    pipe.zadd(self.unacked_index_key, time(), delivery_tag) \
  File "/home/mike/.pyenv/versions/3.6.6/envs/official_accounts/lib/python3.6/site-packages/redis/client.py", line 2263, in zadd
    for pair in iteritems(mapping):
  File "/home/mike/.pyenv/versions/3.6.6/envs/official_accounts/lib/python3.6/site-packages/redis/_compat.py", line 123, in iteritems
    return iter(x.items())
AttributeError: 'float' object has no attribute 'items'
```

redis 库由 3.0.1 换成了 2.10.6 [https://stackoverflow.com/questions/53322425/celery-critical-mainprocess-unrecoverable-error-attributeerrorfloat-object](https://stackoverflow.com/questions/53322425/celery-critical-mainprocess-unrecoverable-error-attributeerrorfloat-object)


### 如何利用celery准确地发送提醒通知

假设A代表一类事件，发生事件已经存在数据库中，如果想要在A发生前`xx`时间发送提醒，那么如何设计：
这里有几个比较关键的点：

>①每次从数据库中获取一批事件A，而不是每隔时刻都在获取事件A(这样太耗费资源)，这就有一个问题：两次获取的间隔是多少，我的方案是：由提前多长时间`xx`决定，如果是`xx`是5分钟(分钟级的)，可以设定取事件的间隔为10分钟或1小时，如果`xx`是30分钟(其实是小时级)，可以设置为1小时、2小时，设置的前提是：应当保证时间间隔内可以将这批事件的提醒发送完毕，不然会产生积压，影响下一个批次的执行性能。这个批次任务可由celery beat实现。<br><br>
>②从数据库中取数据的参数如何设置：<br>
已经确定了批次间隔，这样取值就要取是不是要在当前间隔内发送提醒的数据：比如间隔时间为1天,提前提醒的时间为12小时<br>
那么：<br>
A.objects.filter(Q(time>=今天0点+12小时) & Q(time<明天0点+12小时))<br>
这样要保证批次任务必须在[今天0点, 今天0点+12小时) 这个时间段执行完，否则有些事件A发生了，还没有执行提醒，<br>
解决这个问题，其实可以在本批次执行下个批次的任务，比如，明天需要发送提醒的任务，今天已经发送了计划：
A.objects.filter(Q(time>=今天0点+36小时) & Q(time<明天0点+36小时))<br><br>
>③多次提醒的问题如何解决<br>
有这样场景：如果我在A发生前12小时提醒一次，发生前3天提醒一次<br>
根据①可以设定这个批次间隔为1天，这样对事件A的时间进行夹逼判断：如果<br>
今天0点 <= A.time - 12小时 < 明天0点， 计划发送提醒<br>
今天0点 <= A.time - 3天   <明天0点，计划发送提醒<br>
这里的区间`[今天0点，明天0点)`实际上是由批次间隔决定的，具体的间隔又需要根据多次提醒提前的时间做出调整。计划发送可由celery发送任务的eta参数指定。<br>
如果像②中提到的计划明天需要发送的提醒，那么就得这样处理：<br>
今天0点 + 24小时 <= A.time - 12小时 < 明天0点 + 24小时， 计划发送提醒<br>
今天0点 + 24小时<= A.time - 3天   <明天0点 + 24小时，计划发送提醒<br>
这样又有缺陷，比如刚刚设定time值，而依据time需要今天下午提醒，而此时今天的批次任务已经全部跑完，那么，这个提醒永远也不会发送了。具体的时间还要依据场景不断调整。

















