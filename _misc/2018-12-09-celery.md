---
layout: cnblog_post
title:  "celery"
permalink: '/misc/celery'
date:   2018-12-09 07:34:39
categories: misc
---

<img src="https://raw.githubusercontent.com/iampythoner/iampythoner.github.io/master/static/img/AMQP%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E5%9B%BE.png" width="500" alt="AMQP"/>

<img src="https://raw.githubusercontent.com/iampythoner/iampythoner.github.io/master/static/img/Celery%E6%9E%B6%E6%9E%84%E5%9B%BE.png" width="500" alt="Celery架构图"/>

#### 基本使用

```
In : from proj.tasks import add
In : r = add.delay(1, 3)
In : r
Out: <AsyncResult: xxxxx>

In : r.result
Out: 4

In : r.status
Out: u'SUCCESS'

In : r.successful()
Out: True

In : r.backend
Out: <celery.backends.redis.RedisBackedn at 0x78943> # 保存在redis中


In : task_id = 'xxxxxx'
In : add.AsyncResult(task_id).get() # 获取指定task_id的结果
Out: 4

# 或者
In : from celery.result import AsyncResult
In : AsyncResult(task_id).get()
Out: 4
```

#### 使用任务调度 beat

```
CELERYBEAT_SCHEDUEL = {
    "task": 'projb.tasks.add',
    'schedule': timedelta(seconds=10),
    'args': (16, 16)
}
```

指定了任务每10秒跑一次，同时指定了参数16 和 16，

启动beat:

```
celery beat -A projb
```
然后启动worker。

Beat 和 worker 可以一起启动：

```
celery -B -A projb worker -l info
```

#### 任务绑定、记录日志和重试

任务绑定、记录日志和重试是Celery常用的3个高级属性。现在修改proj/tasks.py文件，添加div函数用于演示:

```
from celery.utils.log import get_task_logger

logger = get_task_logger(_ name_ _)

@app.task(bind=True)
def div(self, x, y):
    logger.info(('Executing task id {0.id}, args: {0.args!r}'
    'kwargs: {0.kwargs!r}'). format(self.request))

    try:
        result = x / y
    except ZeroDivisionError as e:
        raise self.retry(exc=e, countdown=5, max_retries=3)
    return result
```

当使用bind = True后，函数的参数发生变化，多出了参数self(第一个参数)， 相当于把变成了一个已绑定的方法，通过self可以获得任务的上下文。<br>
在IPython中调用div:

```
In : from proj.tasks import div
In : r = div.delay(2, 1)
```

可以看到如下执行信息:

```
[2016-06-03 15:50:32,853: INFO/Worker-1] proj.tasks.div[71606-835-1311282-45-856B8-2]: Executing task id 71606-835-1311282-45-856B8-2, args: [2, 1] kwargs {}
```

换成能造成异常的参数:

```
In: r = div.delay(2, 0)
```

可以发现每5秒就会重试一次， -共重试3次(默认重复3次),然后抛出异常。


#### 任务调用

之前我们调用任务都使用`delay`方法，格式如下:

```
task.delay(arg1, arg2, kwarg1='x', kwarg2='y')
```

`delay`其实是`apply_async`的别名，还可以使用如下方法调用

```
task.apply_async(args=[arg1, arg2], kwargs={'kwarg1':'x', 'kwarg2'='y'}) 
```

二者的区别在于参数的表达方式，而且apply_async支持更多的参数:

```
1. countdown: 等待-段时间再执行。
add.apply_async((2, 2)， countdown=5) # 5秒后再执行

2. eta: ETA是estimated time of arrival 的缩写，也就是定义了任务的开始时间。countdown相当于特殊的单位为秒的eta。
add.apply_ async((2, 2)， eta=now + timedelta(seconds=10)) # 时间到了now加了10秒，任务才开始

3. expires: 设置超时时间。
add.apply_async((2, 2), expires=60) # 60秒之后过期

4. retry: 定义如果任务失败是否重试。
add.apply_async((2, 2), retry=Fase) # 失败了不会重试

5. retry_policy:重试策略。 可以设置如下几个键。
```

| 键 | 含义 |
| ----- | ----- |
| max_retries | 最大重试次数，默认为3次 |
| interval_start | 重试等待的间隔秒数，默认为0.表示直接重试不等待 |
| interval_step | 每次重试让重试间隔增加的秒数，比如第1次重试间隔0秒，第二次重试间隔就是0.2秒，... 可以是数数字或者浮点数，默认值是0.2 |
| interval_max | 重试间隔最大的秒数。也就是通过interal_step增大到多少秒之后就不再增加了。可以是数字或者浮点数，默认值是0.2 |


使用apply_ async 也可以自定义发布者、交换机、路由键、队列、优先级(目前只支持Redis和Beanstalk)、序列方案和压缩方法:

```
add.apply_async((2, 2), compression= 'zlib',
    serializer= 'json', queue='priority.high',
    routing_key='web.add', priority=0, exchange='web_exchange')
```

#### 信号系统

参见 [http://docs.celeryproject.org/en/latest/userguide/signals.html#task-signals](http://docs.celeryproject.org/en/latest/userguide/signals.html#task-signals)


##### 任务信号

| 信号 | 含义 |
| ----- | ----- |
| before_task_publish | 任务发布前|
| after_task_publish | 任务发布后|
| task_prerun | 任务执行前 |
| task_postrun | 任务执行后 |
| task_retry | 任务重试时 |
| task_success | 任务完成时 | 
| task_failure | 任务失败时 |
| task_revoked | 任务撤销或终止 |
| task_unknown | worker接收到没有注册的消息 |
| task_rejected | 当worker接收到位置类型的消息时 |


```
from celery.signal import after_task_publish

@after_task_publish.connect
def task_sent_handler(sender=None, body=None, **kwargs):
    print('after_task_publish： task_id: {body[id]}; sender: {sender}').format(
        body=body, sender=sender
    )


# 结果
after_task_publish： task_id: xxxx; sender: proj.tasks.add
```
信号可以帮助我们了解任务执行情况，分析任务运行的瓶预。

##### 应用信号

##### worker信号

##### Beat信号

##### Eventlet信号

##### 日志信号

##### 命令信号

#### worker 管理

通常任务启动的方式：

```
/home/xuetangapp/.virtualenvs/mobile-api/bin/celery -A push.celeryapp worker -l info --concurrency=6 -Q group_send_queue,single_send_queue
```

--concurrency参数会忽略在初始化app时配置指定的concurrency参数 <br>
-Q 指定的两个队列，前者为高优先级 （除了指定的队列的任务，其他任务这个worker不处理）<br>

想要以Daemon方式启动worker进程:

```
celery multi start label_name -A celeryapp -l info --pid=./celery_%n.pid --logfile=./celery_%n.log
```

其他格式符号:

| 参数 | 含义 |
| ----- | ----- |
| %n | 只包含主机名 |
| %h | 包含域名的主机名 |
| %d | 只包含域名 |
| %i | Prefork类型的进程索引，如果是主进程，则为0 |
| %I | 带分隔符的Prefork类型的进程索引。假设主进程是worker1，那么进程池的第一个进程为worker1-1 |

```
celery multi show label_name # 查看label_name 启动时的命令                          # 输出                                           
/path/to/bin/python -m celery worker --detach -n push@MikedeMBP.lan --pidfile=label_name.pid --logfile=label_name%I.log --executable=/path/to/bin/python 

celery multi names label_name # 获取label_name的节点名字
# 输出
label_name@MikedeMBP.lan

celery multi stop label_name # 停止label_name进程
# 输出
> Stopping nodes...
        > label_name@MikedeMBP.lan: TERM -> 17574
Could not signal label_name@MikedeMBP.lan (17574): No such process
> label_name@MikedeMBP.lan: DOWN

celery multi restart label_name  # 重新启动label_name 
# 输出
celery multi v4.2.1 (windowlicker)
> Stopping nodes...
        > label_name@MikedeMBP.lan: TERM -> 17574
Could not signal label_name@MikedeMBP.lan (17574): No such process
> Restarting node label_name@MikedeMBP.lan: OK


celery multi kill label_name   # 杀掉label_name 进程 （杀掉的是master进程，不要使用这个命令，因为使用之后master不在，无法管理其他worker进程，因此用stop命令也无法关闭所有的进程）    
celery multi v4.2.1 (windowlicker)
Sending KILL to node label_name@MikedeMBP.lan (20803)

```


#### 监控和管理celery

Celery提供了一此方便监控和管理的命令， 常用的命令有如下4个。

##### shell

```
1. shell: celery也提供了一个Python交互环境，内置了Celery 应用实例和全部已注册
的任务，支持默认的Python解释器、IPython 和BPython。

> celery shell -A proj
In [1]: celery
Out[1]: <Celery proj:8x7fad4aef8298>
In [2]: add.delay(1, 2) # 可以直接使用而不需要import

2. result: 通过task_id在命令行获得执行结果。
> celery -A proj result eba7fc8b-3c16-4337-998c-d53a4c176a493

3. inspect active:列出当前正在执行的任务。
> celery -A proj inspect active

4. inspect stats:列出Worker的统计数据，常用来查看配置是否正确以及系统的使用情况。
> celery -A proj inspect stats
```

Celery还可以撤销任务:

```
In : rs = add.delay(1, 2)
In : rs.revoke() # 只是撤销，如果任务已经在执行则撒销无效
In : rs.task_id
Out: d24a83e8-e985-48ef-adab-779559

In : app.control.revoke('d24a83e8-e985-48ef-adab-779559') # 通过task_id撤销
In : app.control.revoke('d24a83e8-e985-48ef-adab-779559', terminate=True) # 撤销正在执行的任务，默认使用TERM信号
In : app.control.revoke('d24a83e8-e985-48ef-adab-779559', terminate=True, signale='SIGKILL') # 撤销正在执行的任务，使用KILL信号
In : app.control.revoke([
...:  'd9878da5-9915-40a0-bfa1-392c7bde42ed',
...:  'd24a83e8-e985-48ef-adab 77944e5557',
...:  '444-d14-4e3d 9d2C8B6d'])  # 可以同时撒销多个任务
```

Worker进程在内存中保存了撒销的任务，一重启就会丢失，如果需要这个历史记录持久化，可以添加--statedb参数启动Worker: 

```
celery -A proj worker -l info --statedb=/tmp/worker.state
```

Celery官方推荐实时的Web监控工具Flower, 它实现下特性:

>●可以看到任务历史、任务具体的参数、开始时间等。<br>
>●提供图表和统计数据。<br>
>●实现全面的远程控制功能，包括但不限于撤销/终止任务、关闭和重启Worker进程、查看正在运行的任务等。<br>
>●提供一个HTTP API,方便集成到运维系统中。

同时，Celery 也自带了一个事件监控工具显示任务历史等信息，可以用来检查任务和跟踪错误:

```
celery -A projb events
```
需要注意是，要把设置`CELERY_SEND_TASK_SENT_EVENT`为True才可以获取事件. <br>
事件监控工具默认每秒都会刷新终端，从消息代理以celeryev开头的队列中找到执行任务的历史记录，可以使用键盘按键查看任务信息、错误堆栈、执行结果和撤销任务等。

#### 子任务

我们可以把任务通过签名的方法传给其他任务，成为一个子任务：

```
In : from celery import signature
In : task = signature('tasks.add', args=(2, 2), countdown=10)
Out: tasks.add(2, 2) # 通过签名生成了任务
In : task.apply_async()
Out: <AsnycResult: xxxx>
```
还可以通过如下方式创建子任务:

```
In : from proj.tasks import add
In : task = add.subtask((2, 2), countdown=10) #可以使用
In : task.apply_async()
0ut: <AsyncResult: a402fe1d-ed11-48f8-b163-8d7f65abBe3>
```

`add.subtask` 同样可以使用快捷方式`add.s(2, 2, countdown=10)`。<br>
子任务实现偏函数(partial)的方式非常有用，这种方式可以让任务在传递过程中才传入参数。

```
In : partial = add.s(2)
In : partialapply asyc((4,)) 
Out: <AsyncResult: b3aaa2ce-eddb-44eb-80fb-48ce5e36da00>
```
子任务支持如下5种原语来实现工作流。原语表示由若干条指令组成的，用于完成一定功能的过程。

##### 1. chain: 调用链。

前面的执行结果作为参数传给后面的任务，直到全部完成。

```
In : from celery import chain
In : res = chain(add.s(2, 2)，add.s(4), add.s(8))()
In : res.get()
Out: 16
```

chain也可以使用管道(\|):

```
In: (add.s(2, 2) | add.s(4) | ads(8))().get()
Out: 16
```

##### 2. group: 一次创建多个(一组)任务。

```
In : from celery import group
In : res = group(add.s(i, i) for i in range(10))()
In : res.get()
Out: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
```

##### 3. chord:等待任务全部完成时添加个回调任务。

```
In : res = chord((add.s(i, i) for i in range(1)), add.s(['a']))()
In : rs.get() # 执行完前面的循环，把结果拼成一个列表之后，再对这个列表添加'a'
0ut: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, u'a']
```

##### 4. map/starmap: 每个叁数都作为任务的参数执行一遍，map的参数只有一个，starmap支持的参数有多个

```
In: ~add.starmap(zip(range(10), range(10)))
0ut:[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
```
它相当于:

```
@app.task
def temp():
    return [add(i, i) for i in range(10)]
```
    
##### 5. chunks: 将任务分块


```
In: res = add.chunks(zip(range(50), range(50)), 10)()
In : res.get()
Out:
[[0, 2, 4, 6, 8, 10, 12, 14, 16, 18],
 [20, 22, 24, 26, 28, 30, 32, 34, 36, 38],
 [40, 42, 44, 46, 48, 50, 52, 54, 56, 58],
 [60, 62, 64, 66, 68, 70, 72, 74, 76, 78],
 [80, 82, 84, 86, 88, 90, 92, 94, 96, 98]]
```

#### 进阶篇: Celery 最佳实践

##### 使用自动扩展
多进程和Gevent模式的Worker支持自动扩展，通过--autoscale参数就可以实现:

```
celery -A proj worker -l info --autoscale=6,3
```

--autoscale参数接受2个数字。在上面的句子中，`autoscale=6,3`表示进程池平时保持3个进程，最大并发进程数可以达到6个。这里的取值和你的服务器CPU个数、任务闲忙程度、可用内存等指标有关。笔者个人的习惯是将最大并发数设置为CPU个数的2倍。理论上Worker进程数与CPU个数接近即可，但是也不尽然，它还与你的任务类型有关。比如笔者曾经遇到过的一个产品线的Worker数就比CPU数高很多，原因是每天上千万的任务中有大量的小任务，也有耗时比较久的任务，而且添加的任务会有瞬时高峰，比如每小时的整点、偶数小时的整点等。这很容易造成多个消费者在处理耗时的任务，让整体的及时性受到影响，而使用自动扩展可以比较好地解决这个问题。

##### 善用远程Debug
Celery 支持远程使用pdb调试任务，非常方便。

先添加一个任务:

```
from clery.contrib import rdb

@app.task
def sub(x, y):
    result = x - y
    rdb.set_trace() # 设置断点
    return result
```
重启Worker进程之后，使用Ipython发布任务:

```
In : from proj.tasks import sub
In : sub(2, 1)
Remote Debugger:6899, Please telnet into 127.0.0.1 6899.

Type 'exit' in ssion to continue.

Remote Debugger:6899 Waiting for client...
```

这个时候我们开启一个新的终端，使用tenet连接到6899端口

```
> telnet localhost 6899
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'
> /home/ubuntu/web develop//tepertstiono/proj/tasks.py(27)sub()
-> return result
(Pdb) result
1
(Pdb) x
2
(Pdb) continue
Connection closed by foreign host.
```

通过这样的方式，就不用在本地搭建Celery环境模拟任务失败的环境了。

##### 合理安排任务周期
如果项目中有很多调度模式的任务，就要合理安排这些定时任务的执行时间。很多人会懒，随便选择一些整点和凌晨期间作为任务执行时间。但是需要注意如下问题:<br>
1.需要和系统管理员和数据库管理员确认，确保你选择的时间段里面没有一些不合时宜的时间点。因为通常在凌晨，尤其是过了12点之后也是大量定时任务运行的时间，比如数据备份、生成前一天的数据报表这样的任务，如果选在凌晨执行你的任务，既可能让你的任务失败，也可能由于对数据库的访问压力让你和别人的任务花费更多的时间。<br>
2.选择非整点的任务执行周期。根据任务特性，可以把任务执行时间打散，尽量减少每小时里前45分钟Celery集群非常忙碌，后15分钟集群大部分空闲的情况。<br>
3.合理安排数据库、文件系统写任务。笔者的方式是把占用时间的数据库和文件系统写操作等任务安排在访问低峰期，并把写任务拆分后在分散的时间执行。

##### 合理使用队列和优先级
不要把任务都放在默认队列。合理安排任务的优先级和队列，让应该及时完成的任务不会因为某些原因被阻寒而影响用户体验。其次是合理使用apply_async 方法临时性地切换队列和优先级，提高整体的响应。
如果能明确知晓某些任务耗时很长，某些任务托时很短，至少可以按照任务花费的时间来把任务安排到不同的队列。

##### 保证业务逻辑的事务性
Celery 虽然提供错误重试机制，但是并没有提供任务的事务性。如果任务一部分执行成功之后失败，执行成功的那部分是没有回滚方法的，所以一开始就要在实现逻辑的时候对重试机制有明确的理解。

##### 关闭你不想要的功能
如果你对任务的执行结果没有兴趣，可以关闭它:

```
@app.task(ignore_result=True)
def mytask(...):
    doit()
```

如果项目不需要限速，可以设置`CELERY_DISABLE_RATE_LIMITS=True`直接全局地把这个特性关闭。

##### 使用阅后即焚模式

在使用队列的时候，默认使用持久化方式以确保任务被执行，如果你的任务不需要持久化，可以使用阅后即焚( transient)模式:

```
from kombu import Queue
Queue('transient', routing_key='transient', delivery_mode=1)
```

##### 善用Prefetch模式
Worker进程默认每次从消息代理获取`CELERYD_PREFETCH_MULTIPLIER`设置的任务数，假如你的任务都很细小，可以修改`CELERYD_PREFETCH_MULTIPLIER`的值，每次获取更多的任务数。这个值也是根据任务的执行情况决定的。

##### 善用工作流
如果一个任务有调用链，下一步任务需要等待上一步的结果，就不应该使用同步子任务。举一个抓取某电商网站的爬虫例子，它般分为以下几步:

```
1.获取要抓取的页面。
2.抓取对应页面。
3.解析页面数据。
4.把需要的数据存储到数据库和缓存中。
```

如果设计成下面的方式就是错误的:

```
@app.task
def page_crawler();
    url = get_url.delay().get()
    page = fetch_page.delay(url).get()
    info = parse_page.delay(page).get()
    store_page_info.delay(info)

@app.task
def get_url():
return PageInfo.objects.filter_by(need_upDate=True).first()

@app.task
def fetch_page(url):
    return requests.get(url)

@app.task
def parse_page(page):
    return myparser.parse_document(page)

@app.task
def store_page_info(info):
    return PageInfo.objects.create(info)
```
    
应该使用如下方式:

```
def page_crawler():
    # get_url -> fetch_page -> parse. page -> store page
    chain = get_url.s() I fetch_page.s() | parse_page.s() | store_page_info.s()chain()
```

控制任务的粒度不要太细。举个例子，假设现在每小时都要通过某网站的开放平台获取应的百万级条目的数据。如果为每个条目都创建一个任务显然是不合理的，应该更多地利用开放平台接口的限制。比如接口可以批量操作，一次最多取30个条目的数据，那么可考虑把单个任务定为处理30个条目的数据。

在生成任务的时候应该充分利用group/chain/chunks这些原语。



















